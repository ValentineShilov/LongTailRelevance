{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IBemtNWUq3Uu",
    "outputId": "ec3be2c0-bec4-412e-e514-bb0fff69f20c"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ceRQBdxzVde-"
   },
   "outputs": [],
   "source": [
    "!cp \"../data/normal_titles.tx\" ./\n",
    "!cp \"../data/normal_queries.txt\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXmhsxoUUthJ"
   },
   "outputs": [],
   "source": [
    "titles_corpus = []\n",
    "dids = []\n",
    "didToLine = {}\n",
    "titles=[]\n",
    "with open(\"normal_titles.txt\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        did,title = line.split(\"\\t\")\n",
    "        did = int(did)\n",
    "        words = title.split(\" \")\n",
    "        titles.append(title)\n",
    "        titles_corpus.append(words)\n",
    "        dids.append(did)\n",
    "        didToLine[did] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKVYXhmsXjWN"
   },
   "outputs": [],
   "source": [
    "qidToTopTitle = {}\n",
    "with open(\"top_titles2.txt\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "     \n",
    "      line = line.strip()\n",
    "      parts = line.split(maxsplit=1)\n",
    "      if len(parts)==2:\n",
    "          qid = int(parts[0])\n",
    "          title = parts[1]\n",
    "          qidToTopTitle[qid] = title.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JkhXghPyX8RY",
    "outputId": "af5e65fa-f0ab-4855-f677-615902c0ad2f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSM3xUSIV8Xe"
   },
   "outputs": [],
   "source": [
    "queries_corpus = []\n",
    "qids = []\n",
    "qidToLine = {}\n",
    "queries = []\n",
    "with open(\"normal_queries.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        qid,title = line.split(\"\\t\")\n",
    "        qid = int(qid)\n",
    "        words = title.split(\" \")\n",
    "        queries.append(title)\n",
    "        queries_corpus.append(words)\n",
    "        qidToLine[qid] = len(qids)\n",
    "        qids.append(qid)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3QFWGmjWB4j"
   },
   "outputs": [],
   "source": [
    "def getQuery(did):\n",
    "    global queries\n",
    "    global qidToLine\n",
    "    if qid in  qidToLine:\n",
    "        return queries[qidToLine[qid]]\n",
    "    return \"\"\n",
    "def getTitle(did):\n",
    "    global titles\n",
    "    global didToLine\n",
    "    if did in  didToLine:\n",
    "        return titles[didToLine[did]]\n",
    "    return \"\"\n",
    "def getTitlesForQuery(qid):\n",
    "    global titles\n",
    "    global didToLine\n",
    "    global allQidToDocs\n",
    "    rv=[]\n",
    "    dids = []\n",
    "    if qid in allQidToDocs:\n",
    "        for did in allQidToDocs[qid]:\n",
    "            if did in  didToLine:\n",
    "                rv.append(titles[didToLine[did]])\n",
    "                dids.append(did)\n",
    "    return rv, dids  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Do5VKewBWEFh"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "def parseTrainTestFile(train_marks_filename, isTest=False):\n",
    "\n",
    "    trainDidToLine = {}\n",
    "    qidToDocs = defaultdict(list)\n",
    "    \n",
    "    with open(train_marks_filename) as f:\n",
    "        i=0\n",
    "        nlines = 0\n",
    "        for line in f:\n",
    "            nlines+=1\n",
    "        \n",
    "       \n",
    "        qids = np.zeros( nlines, dtype=int)\n",
    "        i=0\n",
    "        f.seek(0)\n",
    "        if isTest:\n",
    "            f.readline()\n",
    "        for ii,line in enumerate(f):\n",
    "            if not isTest:\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts)!=3:\n",
    "                    continue\n",
    "                qid, did, mark = parts\n",
    "                qid, did, mark = int(qid), int(did), int(mark)\n",
    "            else:\n",
    "                parts = line.split(\",\")\n",
    "                if len(parts)!=2:\n",
    "                    continue\n",
    "                qid, did = parts\n",
    "                mark=-1\n",
    "                qid, did, mark = int(qid), int(did), int(mark)\n",
    "\n",
    "            qids[i] = qid\n",
    "            trainDidToLine[(qid,did)] = i\n",
    "\n",
    "            qidToDocs[qid].append(did)\n",
    "            i+=1\n",
    "    return qids, trainDidToLine, qidToDocs\n",
    "q_train, trainDidToLine, trainQidToDocs = parseTrainTestFile(\"train.marks.tsv\", \n",
    "                                                    isTest=False, \n",
    "                                                    )\n",
    "\n",
    "q_test, testDidToLine, testQidToDocs = parseTrainTestFile(\"sample.csv\", \n",
    "                                                    isTest=True, \n",
    "                                                    )\n",
    "def url_normalize(url):\n",
    "    url = url.strip()\n",
    "    url = url.replace(\"https://\", \"\")\n",
    "    url = url.replace(\"http://\", \"\")\n",
    "    if len(url)>1:\n",
    "        if url[-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "        url = url.lower()\n",
    "        url = url.replace(\":80\", \"\")\n",
    "        url = url.replace(\"www.\", \"\")\n",
    "        url = url.replace(\"//\", \"\")\n",
    "    return url\n",
    "def extract_domain(url):\n",
    "    global rubrics_list\n",
    "    url = url.strip()\n",
    "    #for r in rubrics_list:\n",
    "        #url = url.replace(\",\" + r + \",\" , \"\")    \n",
    "    #    url = url.replace(\",\" + r, \"\") \n",
    "    url = url.replace(\"https://\", \"\")\n",
    "    url = url.replace(\"http://\", \"\")\n",
    "   \n",
    "    url = url.replace(\"ftp://\", \"\")\n",
    "    if len(url)>1:\n",
    "        if url[-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "        url = url.lower()\n",
    "        \n",
    "        url = url.replace(\"s://\", \"\")\n",
    "        url = url.replace(\":80\", \"\")\n",
    "        url = url.replace(\"://\", \"\")\n",
    "        url = url.replace(\"www.\", \"\")\n",
    "        url = url.replace(\"//\", \"\")\n",
    "        pos = url.find(\"#\")\n",
    "        if pos>0:\n",
    "            url = url[0:pos]\n",
    "\n",
    "        pos = url.find(\"/\")\n",
    "        if pos>0:\n",
    "            url = url[0:pos]\n",
    "    return url\n",
    "def loadUrls(filename):\n",
    "    urlToDid = {}\n",
    "    didToUrl = {}\n",
    "    domainToDid = defaultdict(list)\n",
    "    didToDomain = defaultdict(list)\n",
    "   \n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            uid, url = line.split(\"\\t\")\n",
    "            uid = int(uid)\n",
    "            #urls[\"http://\" + url] = uid\n",
    "            #urls[\"http://\" + url + \"/\"] = uid\n",
    "            urlToDid[url_normalize(url)] = uid\n",
    "            didToUrl[uid] = url_normalize(url)\n",
    "            domain = extract_domain(url)\n",
    "            domainToDid[domain].append(uid)\n",
    "            didToDomain[uid] = domain\n",
    "            \n",
    "    return urlToDid,didToUrl,domainToDid,didToDomain\n",
    "urlToDid,didToUrl,domainToDid,didToDomain = loadUrls(\"url.data\")\n",
    "urls = urlToDid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUZBwFyJWH2Z"
   },
   "outputs": [],
   "source": [
    "allQidToDocs = defaultdict(list)\n",
    "didToQueries = defaultdict(list)\n",
    "for qid,docs in trainQidToDocs.items():\n",
    "    allQidToDocs[qid]+=(docs)\n",
    "    for doc in docs:\n",
    "        didToQueries[doc].append(qid)\n",
    "for qid,docs in testQidToDocs.items():\n",
    "    allQidToDocs[qid]+=(docs)\n",
    "    for doc in docs:\n",
    "        didToQueries[doc].append(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpembLEnWL2S"
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "not_matched = 0\n",
    "nqueries=0\n",
    "\n",
    "def loadTopDocsFile(filename):\n",
    "    found_docs = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            qid,fd = line.split(\",\")\n",
    "            qid = int(qid)\n",
    "            fd = int(fd)\n",
    "            found_docs[qid] = (fd,0.0)\n",
    "    return found_docs\n",
    "found_docs = loadTopDocsFile(\"first_top_docs.txt\")\n",
    "found_domains = loadTopDocsFile(\"first_top_domains.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsBVLRBsmkyJ"
   },
   "outputs": [],
   "source": [
    "questions = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-_1ME5Ew1L-"
   },
   "outputs": [],
   "source": [
    "all_qids = [qid for qid in allQidToDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2vdgoVaEw2zo",
    "outputId": "92761431-49e4-4212-8599-67add5842c1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6311"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7V2lG_JTAe4q"
   },
   "outputs": [],
   "source": [
    "!pip3 install tensorflow_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNolMHErAXw0"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "# library test from examples\n",
    "english_sentences = [\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"]\n",
    "italian_sentences = [\"cane\", \"I cuccioli sono carini.\", \"Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.\"]\n",
    "japanese_sentences = [\"犬\", \"子犬はいいです\", \"私は犬と一緒にビーチを散歩するのが好きです\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContentForQid(qid):\n",
    "    print(\"Getting content for \", qid)\n",
    "    dids = []\n",
    "    contents= []\n",
    "    try:\n",
    "        with open(\"../data/content_by_queries/\" + str(qid) + \".txt\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts)>=3:\n",
    "                    did, title, content = int(parts[0]), parts[1], parts[2]\n",
    "                    dids.append(did)\n",
    "                    contents.append(content)\n",
    "        return contents, dids\n",
    "    except Exception as e:\n",
    "        #print(repr(e))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = [hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\"),\n",
    "          hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"),\n",
    "          hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\"),\n",
    "          hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3'),\n",
    "          hub.load('https://tfhub.dev/google/universal-sentence-encoder-qa/3')\n",
    "         ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-oDRO0feApuQ",
    "outputId": "8e86f523-0dbe-4058-ff0c-8693029b10f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28125972]] [[0.25637293]] [[0.68262917]]\n"
     ]
    }
   ],
   "source": [
    "# library test \n",
    "ru_result = en_result = embed([\"мама мыла раму\"])\n",
    "ru_result2 = embed([\"кабан пришёл домой\"])\n",
    "ru_result3 = embed([\"рама помыла ванну\"])\n",
    "print(np.inner(ru_result, ru_result2), np.inner(ru_result2, ru_result3), np.inner(ru_result, ru_result3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cg0TRgvzsgZv",
    "outputId": "448c807c-fb32-45c9-c937-6df787a1fc95"
   },
   "outputs": [],
   "source": [
    "for eid, embed in enumerate(embeds):\n",
    "    with open(\"USE_out2_\" + str(eid) + \".txt\", \"w\") as f:\n",
    "      flag=True\n",
    "      for qid in all_qids:\n",
    "        try:\n",
    "          if(flag):\n",
    "            qtitles,dids  = getTitlesForQuery(qid)\n",
    "            query = getQuery(qid)\n",
    "            top_score = np.zeros(len(dids))\n",
    "            top_score2 = np.zeros(len(dids))\n",
    "            if eid<=2:\n",
    "                title_emb = embed(qtitles)\n",
    "                query_emb = embed([query])[0]\n",
    "            else:\n",
    "                response_contexts,content_dids = getContentForQid(qid)\n",
    "                title_emb = embed.signatures['question_encoder'](tf.constant([query]))\n",
    "                query_emb = embed.signatures['response_encoder'](input=tf.constant(qtitles), context=tf.constant(response_contexts))\n",
    "                \n",
    "           \n",
    "                \n",
    "            if qid in found_docs:\n",
    "                top_did = found_docs[qid][0]\n",
    "                top_title = getTitle(top_did)  \n",
    "                if eid<=2:\n",
    "                    top_emb = embed([top_title])[0]\n",
    "               \n",
    "                    top_score = np.inner(top_emb, title_emb)\n",
    "            if qid in qidToTopTitle:\n",
    "                top_title = qidToTopTitle[qid] \n",
    "                if eid<=2:\n",
    "                    top_emb = embed([top_title])[0]\n",
    "                \n",
    "                    top_emb = embed.signatures['question_encoder'](tf.constant([top_title]))\n",
    "                    top_score2 = np.inner(top_emb, title_emb)\n",
    "                else:\n",
    "                    pass\n",
    "                #\n",
    "            \n",
    "\n",
    "            score = np.inner(query_emb, title_emb)\n",
    "\n",
    "            for i,s in enumerate(score):\n",
    "              f.write(str(qid) + \",\" + str(dids[i]) + \",\" + str(s) + \",\" + str(top_score[i]) + \",\" + str(top_score2[i]) + \"\\n\")\n",
    "            if qid%100==0:\n",
    "                if qid==1200 :\n",
    "                    flag=True\n",
    "                if(flag):\n",
    "                    print(qid, score)\n",
    "        except Exception as e:\n",
    "            print(repr(e), qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jOhyoDdwgOA"
   },
   "outputs": [],
   "source": [
    "import h2o4gpu as sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_i_B_gXWN5z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getContentForQid(qid):\n",
    "    print(\"Getting content for \", qid)\n",
    "    dids = []\n",
    "    contents= []\n",
    "    try:\n",
    "        with open(\"../data/content_by_queries/\" + str(qid) + \".txt\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts)>=3:\n",
    "                    did, title, content = int(parts[0]), parts[1], parts[2]\n",
    "                    dids.append(did)\n",
    "                    contents.append(content)\n",
    "        return contents, dids\n",
    "    except Exception as e:\n",
    "        #print(repr(e))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../data/res_queries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7wd9_Y4ZDN7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h2o4gpu\n",
    "import h2o4gpu as sklearn\n",
    "from h2o4gpu.feature_extraction.text import TfidfVectorizer\n",
    "from h2o4gpu.metrics.pairwise import cosine_similarity\n",
    "def processQuery(qid):\n",
    "  try:\n",
    "    path = \"../data/res_queries/\" + str(qid) + \".txt\"\n",
    "    \n",
    "    with open(path, \"w\") as f:\n",
    "      \n",
    "        #ts, dids = getTitlesForQuery(qid)\n",
    "    \n",
    "        contents, contentDids = getContentForQid(qid)\n",
    "        dids = contentDids\n",
    "        ts  = contents\n",
    "        thisdidtoline = {}\n",
    "        for ind,tmp in enumerate(dids):\n",
    "            thisdidtoline[tmp] = ind\n",
    "        query = getQuery(qid)\n",
    "\n",
    "        \n",
    "        \n",
    "        TextVectorizerWord  = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,3))\n",
    "        \n",
    "        contentWord = TextVectorizerWord.fit_transform(contents)\n",
    "        contentWordQuery = TextVectorizerWord.transform([query])\n",
    "        contentQuerySim = cosine_similarity(contentWord, contentWordQuery)\n",
    "        \n",
    "        \n",
    "        TextVectorizerNgram = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,12))\n",
    "        contentNgram = TextVectorizerNgram.fit_transform(contents)\n",
    "        contentNgramQuery = TextVectorizerNgram.transform([query])\n",
    "        contentNgramSim = cosine_similarity(contentNgram, contentNgramQuery)\n",
    "        \n",
    "       \n",
    "        sqsim = np.zeros((len(dids),1))\n",
    "        fcsim = np.zeros((len(dids),1))\n",
    "        if qid in found_docs:\n",
    "            fdid,score = found_docs[qid]\n",
    "            if fdid in didToLine:\n",
    "                t1 = getTitle(fdid)\n",
    "                #tidf = vectorizer.fit_transform(ts)\n",
    "                sqidf = TextVectorizerNgram.transform([t1])\n",
    "                sqsim = cosine_similarity(contentNgram, sqidf)\n",
    "                \n",
    "            if fdid in thisdidtoline:\n",
    "                line = thisdidtoline[fdid]\n",
    "                fcontent = contents[line]\n",
    "                fcidf = TextVectorizerWord.transform([fcontent])\n",
    "                fcsim = cosine_similarity(contentWord, fcidf)\n",
    "       \n",
    "        sqsimd = np.zeros((len(dids),1))\n",
    "        fcsimd = np.zeros((len(dids),1))\n",
    "        \n",
    "        if qid in found_domains:\n",
    "            fdid,score = found_domains[qid]\n",
    "            if fdid in didToLine:\n",
    "                t1 = getTitle(fdid)\n",
    "               \n",
    "                sqidf = TextVectorizerNgram.transform([t1])\n",
    "                sqsimd = cosine_similarity(contentNgram, sqidf)\n",
    "                \n",
    "            if fdid in thisdidtoline:\n",
    "                line = thisdidtoline[fdid]\n",
    "                fcontent = contents[line]\n",
    "                fcidf = TextVectorizerWord.transform([fcontent])\n",
    "                fcsimd = cosine_similarity(contentWord, fcidf)\n",
    "       \n",
    "     \n",
    "       \n",
    "        for i in range(len(ts)):\n",
    "            \n",
    "            f.write(str(qid) + \",\" + str(dids[i]) + \",\" + str(contentQuerySim[i,0]) + \",\" + str(contentNgramSim[i,0]) + \",\" + str(fcsim[i,0]) + \",\" + str(sqsim[i,0]) + \",\" + str(fcsimd[i,0]) + \",\" + str(sqsimd[i,0])  + str(\"\\n\") )\n",
    "            f.flush()\n",
    "        \n",
    "  except Exception as e:\n",
    "    #print(repr(e))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cB9_U8zB9fUe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZPuq3gXfZ9bT",
    "outputId": "9766a445-2035-44ef-8fa9-afd06eeb5da7"
   },
   "outputs": [],
   "source": [
    "all_qids = [qid for qid in allQidToDocs]\n",
    "import random\n",
    "random.shuffle(all_qids)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "p = Pool(32)\n",
    "p.map(processQuery, all_qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pjkrrHo3ciww",
    "outputId": "b9e2f5a5-308e-4cc8-940c-5b056e483dc7"
   },
   "outputs": [],
   "source": [
    "from h2o4gpu.feature_extraction.text import TfidfVectorizer\n",
    "from h2o4gpu.metrics.pairwise import cosine_similarity\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tLReMBBEhlg1",
    "outputId": "e8d46f39-a2d8-4f64-efb2-84e6b06c4cef"
   },
   "outputs": [],
   "source": [
    "!cat \"../data/res_queries/*.txt\" >> ../data/tfidf_features.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3e2P1IQh-cy"
   },
   "outputs": [],
   "source": [
    "def mergeFiles(file1, file2, fileout):\n",
    "    a = {}\n",
    "    with open(file1) as f:\n",
    "      for line in f:\n",
    "        line = line.strip()\n",
    "        parts = line.split(\",\")\n",
    "        if len(parts)>2:\n",
    "          qid,did = int(parts[0]), int(parts[1])\n",
    "          data = list(map(float,(parts[2:])))\n",
    "          alen = len(data)\n",
    "          a[(qid,did)] = data\n",
    "    b = {}\n",
    "    with open(file2) as g:\n",
    "        for line in g:\n",
    "          line = line.strip()\n",
    "          parts = line.split(\",\")\n",
    "          if len(parts)>2:\n",
    "            qid,did = int(parts[0]), int(parts[1])\n",
    "            data = list(map(float,(parts[2:])))\n",
    "            blen = len(data)\n",
    "            b[(qid,did)] = data\n",
    "    rd = {}\n",
    "    for (qid,did), data in a.items():\n",
    "       if  (qid,did) not in b:\n",
    "          b[ (qid,did)] = [0.0] * blen\n",
    "    for (qid,did), data in b.items():\n",
    "       if (qid,did) not in a:\n",
    "          a[(qid,did)] = [0.0] * alen\n",
    "    for (qid,did), data in b.items():\n",
    "      rd[(qid,did)] = a[(qid,did)]\n",
    "      rd[(qid,did)] += data\n",
    "    with open(fileout, \"w\") as h:\n",
    "        for (qid,did), data in rd.items():\n",
    "          data = list(map(str,(data)))\n",
    "          h.write(str(qid) +\",\" + str(did) +\",\" + ','.join(data) + \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7grqT5UnQqi4"
   },
   "outputs": [],
   "source": [
    "!cp ../data/tfidf_features.txt ./USE_tfidf_features.txt\n",
    "for i in range(len(embeds)):\n",
    "    mergeFiles(\"USE_out2_\" + str(i) + \".txt\", \"USE_tfidf_features.txt\", \"USE_tfidf_features.txt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "USE_gpu_tfidf-content.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
