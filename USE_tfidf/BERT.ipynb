{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBemtNWUq3Uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXmhsxoUUthJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titles_corpus = []\n",
        "dids = []\n",
        "didToLine = {}\n",
        "titles=[]\n",
        "with open(\"normal_titles.txt\") as f:\n",
        "    for i,line in enumerate(f):\n",
        "        line = line.strip()\n",
        "        did,title = line.split(\"\\t\")\n",
        "        did = int(did)\n",
        "        words = title.split(\" \")\n",
        "        titles.append(title)\n",
        "        titles_corpus.append(words)\n",
        "        dids.append(did)\n",
        "        didToLine[did] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKVYXhmsXjWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qidToTopTitle = {}\n",
        "with open(\"top_titles2.txt\") as f:\n",
        "    for i,line in enumerate(f):\n",
        "     \n",
        "      line = line.strip()\n",
        "      parts = line.split(maxsplit=1)\n",
        "      if len(parts)==2:\n",
        "          qid = int(parts[0])\n",
        "          title = parts[1]\n",
        "          qidToTopTitle[qid] = title.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSM3xUSIV8Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "queries_corpus = []\n",
        "qids = []\n",
        "qidToLine = {}\n",
        "queries = []\n",
        "with open(\"normal_queries.txt\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        qid,title = line.split(\"\\t\")\n",
        "        qid = int(qid)\n",
        "        words = title.split(\" \")\n",
        "        queries.append(title)\n",
        "        queries_corpus.append(words)\n",
        "        qidToLine[qid] = len(qids)\n",
        "        qids.append(qid)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3QFWGmjWB4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getQuery(did):\n",
        "    global queries\n",
        "    global qidToLine\n",
        "    if qid in  qidToLine:\n",
        "        return queries[qidToLine[qid]]\n",
        "    return \"\"\n",
        "def getTitle(did):\n",
        "    global titles\n",
        "    global didToLine\n",
        "    if did in  didToLine:\n",
        "        return titles[didToLine[did]]\n",
        "    return \"\"\n",
        "def getTitlesForQuery(qid):\n",
        "    global titles\n",
        "    global didToLine\n",
        "    global allQidToDocs\n",
        "    rv=[]\n",
        "    dids = []\n",
        "    if qid in allQidToDocs:\n",
        "        for did in allQidToDocs[qid]:\n",
        "            if did in  didToLine:\n",
        "                rv.append(titles[didToLine[did]])\n",
        "                dids.append(did)\n",
        "    return rv, dids  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do5VKewBWEFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "def parseTrainTestFile(train_marks_filename, isTest=False):\n",
        "\n",
        "    trainDidToLine = {}\n",
        "    qidToDocs = defaultdict(list)\n",
        "    \n",
        "    with open(train_marks_filename) as f:\n",
        "        i=0\n",
        "        nlines = 0\n",
        "        for line in f:\n",
        "            nlines+=1\n",
        "        \n",
        "       \n",
        "        qids = np.zeros( nlines, dtype=int)\n",
        "        i=0\n",
        "        f.seek(0)\n",
        "        if isTest:\n",
        "            f.readline()\n",
        "        for ii,line in enumerate(f):\n",
        "            if not isTest:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts)!=3:\n",
        "                    continue\n",
        "                qid, did, mark = parts\n",
        "                qid, did, mark = int(qid), int(did), int(mark)\n",
        "            else:\n",
        "                parts = line.split(\",\")\n",
        "                if len(parts)!=2:\n",
        "                    continue\n",
        "                qid, did = parts\n",
        "                mark=-1\n",
        "                qid, did, mark = int(qid), int(did), int(mark)\n",
        "\n",
        "            qids[i] = qid\n",
        "            trainDidToLine[(qid,did)] = i\n",
        "\n",
        "            qidToDocs[qid].append(did)\n",
        "            i+=1\n",
        "    return qids, trainDidToLine, qidToDocs\n",
        "q_train, trainDidToLine, trainQidToDocs = parseTrainTestFile(\"train.marks.tsv\", \n",
        "                                                    isTest=False, \n",
        "                                                    )\n",
        "\n",
        "q_test, testDidToLine, testQidToDocs = parseTrainTestFile(\"sample.csv\", \n",
        "                                                    isTest=True, \n",
        "                                                    )\n",
        "def url_normalize(url):\n",
        "    url = url.strip()\n",
        "    url = url.replace(\"https://\", \"\")\n",
        "    url = url.replace(\"http://\", \"\")\n",
        "    if len(url)>1:\n",
        "        if url[-1] == \"/\":\n",
        "            url = url[:-1]\n",
        "        url = url.lower()\n",
        "        url = url.replace(\":80\", \"\")\n",
        "        url = url.replace(\"www.\", \"\")\n",
        "        url = url.replace(\"//\", \"\")\n",
        "    return url\n",
        "def extract_domain(url):\n",
        "    global rubrics_list\n",
        "    url = url.strip()\n",
        "    #for r in rubrics_list:\n",
        "        #url = url.replace(\",\" + r + \",\" , \"\")    \n",
        "    #    url = url.replace(\",\" + r, \"\") \n",
        "    url = url.replace(\"https://\", \"\")\n",
        "    url = url.replace(\"http://\", \"\")\n",
        "   \n",
        "    url = url.replace(\"ftp://\", \"\")\n",
        "    if len(url)>1:\n",
        "        if url[-1] == \"/\":\n",
        "            url = url[:-1]\n",
        "        url = url.lower()\n",
        "        \n",
        "        url = url.replace(\"s://\", \"\")\n",
        "        url = url.replace(\":80\", \"\")\n",
        "        url = url.replace(\"://\", \"\")\n",
        "        url = url.replace(\"www.\", \"\")\n",
        "        url = url.replace(\"//\", \"\")\n",
        "        pos = url.find(\"#\")\n",
        "        if pos>0:\n",
        "            url = url[0:pos]\n",
        "\n",
        "        pos = url.find(\"/\")\n",
        "        if pos>0:\n",
        "            url = url[0:pos]\n",
        "    return url\n",
        "def loadUrls(filename):\n",
        "    urlToDid = {}\n",
        "    didToUrl = {}\n",
        "    domainToDid = defaultdict(list)\n",
        "    didToDomain = defaultdict(list)\n",
        "   \n",
        "    with open(filename, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            uid, url = line.split(\"\\t\")\n",
        "            uid = int(uid)\n",
        "            #urls[\"http://\" + url] = uid\n",
        "            #urls[\"http://\" + url + \"/\"] = uid\n",
        "            urlToDid[url_normalize(url)] = uid\n",
        "            didToUrl[uid] = url_normalize(url)\n",
        "            domain = extract_domain(url)\n",
        "            domainToDid[domain].append(uid)\n",
        "            didToDomain[uid] = domain\n",
        "            \n",
        "    return urlToDid,didToUrl,domainToDid,didToDomain\n",
        "urlToDid,didToUrl,domainToDid,didToDomain = loadUrls(\"url.data\")\n",
        "urls = urlToDid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUZBwFyJWH2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allQidToDocs = defaultdict(list)\n",
        "didToQueries = defaultdict(list)\n",
        "for qid,docs in trainQidToDocs.items():\n",
        "    allQidToDocs[qid]+=(docs)\n",
        "    for doc in docs:\n",
        "        didToQueries[doc].append(qid)\n",
        "for qid,docs in testQidToDocs.items():\n",
        "    allQidToDocs[qid]+=(docs)\n",
        "    for doc in docs:\n",
        "        didToQueries[doc].append(qid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpembLEnWL2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnt=0\n",
        "not_matched = 0\n",
        "nqueries=0\n",
        "found_docs = {}\n",
        "found_domains = {}\n",
        "def loadTopDocsFile(filename):\n",
        "    found_docs = {}\n",
        "    with open(filename, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            qid,fd = line.split(\",\")\n",
        "            qid = int(qid)\n",
        "            fd = int(fd)\n",
        "            found_docs[qid] = (fd,0.0)\n",
        "    return found_docs\n",
        "found_docs = loadTopDocsFile(\"first_top_docs.txt\")\n",
        "found_domains = loadTopDocsFile(\"first_top_domains.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHZTFWg7mifh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "c9e4cec8-b171-4561-cd36-5a96f4f080cd"
      },
      "source": [
        "!pip install -U bert-serving-server[http]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-serving-server[http]\n",
            "  Using cached https://files.pythonhosted.org/packages/b0/bd/cab677bbd0c5fb08b72e468371d2bca6ed9507785739b4656b0b5470d90b/bert_serving_server-1.10.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: GPUtil>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (19.0.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.12.0)\n",
            "Collecting flask-compress; extra == \"http\"\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/96/cd684c1ffe97b513303b5bfd4bbfb4114c5f4a5ea8a737af6fd813273df8/Flask-Compress-1.5.0.tar.gz\n",
            "Collecting flask-cors; extra == \"http\"\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: flask; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: bert-serving-client; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from bert-serving-server[http]) (1.10.0)\n",
            "Collecting flask-json; extra == \"http\"\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/2d/4c21d98b11f3a206fabbdd965b53a2ca3ee9fab7646c93cf36c060e8f1a4/Flask_JSON-0.3.4-py3-none-any.whl\n",
            "Collecting brotli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/68/60a220454dc5083c6d59b41aa90bb1c96fad62a0abf3a33e0ef64b38638a/Brotli-1.0.7-cp36-cp36m-manylinux1_x86_64.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (2.11.2)\n",
            "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask; extra == \"http\"->bert-serving-server[http]) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask; extra == \"http\"->bert-serving-server[http]) (1.1.1)\n",
            "Building wheels for collected packages: flask-compress\n",
            "  Building wheel for flask-compress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flask-compress: filename=Flask_Compress-1.5.0-cp36-none-any.whl size=5273 sha256=710824ebe4826b213801caf96e11afa60b3b364a38effa79acc914306ebe14d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/e9/e4/5afc286be7c87461375e33152558415dfeb0c8f5af3b50e742\n",
            "Successfully built flask-compress\n",
            "Installing collected packages: brotli, flask-compress, flask-cors, flask-json, bert-serving-server\n",
            "Successfully installed bert-serving-server-1.10.0 brotli-1.0.7 flask-compress-1.5.0 flask-cors-3.0.8 flask-json-0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVwJYpiDAvan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#дальше скачайте файлы моделей, запустите BERT сервер и выполните этот ноутбук для каждой модели по отдельности"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGxMkLT5mkl2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9cd3f254-aaeb-43a0-8e50-27a1abadcb94"
      },
      "source": [
        "!pip install bert-serving-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-serving-client in /usr/local/lib/python3.6/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (1.18.5)\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (19.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsBVLRBsmkyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions = queries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFjSxJM_n1S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from bert_serving.client import BertClient\n",
        "bc = BertClient()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15843bGNwrh4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "5ebd2fba-d0ac-4b0f-d790-20eaa1b83de1"
      },
      "source": [
        "a = bc.encode(['First do it', 'then do it right', 'then do it better'])\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.49155793,  0.08796163,  0.08263294, ...,  1.0980438 ,\n",
              "         0.411266  , -0.2539694 ],\n",
              "       [-0.07413446, -0.22783582, -0.08978209, ...,  1.5093104 ,\n",
              "         1.3512006 , -0.03158369],\n",
              "       [-0.2657499 ,  0.1913579 , -0.35613576, ...,  1.3962182 ,\n",
              "         1.4187475 ,  0.12651818]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-_1ME5Ew1L-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_qids = [qid for qid in allQidToDocs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vdgoVaEw2zo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92761431-49e4-4212-8599-67add5842c1f"
      },
      "source": [
        "len(all_qids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6311"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_i_B_gXWN5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getContentForQid(qid):\n",
        "    print(\"Getting content for \", qid)\n",
        "    dids = []\n",
        "    contents= []\n",
        "    try:\n",
        "        with open(\"/content/content_by_queries/\" + str(qid) + \".txt\") as f:\n",
        "            for line in f:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts)>=3:\n",
        "                    did, title, content = int(parts[0]), parts[1], parts[2]\n",
        "                    dids.append(did)\n",
        "                    contents.append(content)\n",
        "        return contents, dids\n",
        "    except Exception as e:\n",
        "        #print(repr(e))\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB9_U8zB9fUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/LongTailRelevance/bert_res/BERT_out4_fixed.txt\", \"w\") as f:\n",
        "  flag=True\n",
        "  for qid in all_qids:\n",
        "    try:\n",
        "      if(flag):\n",
        "        qtitles,dids  = getTitlesForQuery(qid)\n",
        "        query = getQuery(qid)\n",
        "        top_score = np.zeros(len(dids))\n",
        "        top_score2 = np.zeros(len(dids))\n",
        "        title_emb =  bc.encode(qtitles)\n",
        "        if qid in found_docs:\n",
        "          top_did = found_docs[qid][0]\n",
        "          top_title = getTitle(top_did)  \n",
        "          top_emb =  bc.encode([top_title])[0]\n",
        "          \n",
        "          top_score =  np.sum(top_emb * title_emb, axis=1) / np.linalg.norm(title_emb, axis=1)\n",
        "        if qid in qidToTopTitle:\n",
        "         \n",
        "          top_title = qidToTopTitle[qid] \n",
        "          top_emb =  bc.encode([top_title])[0]\n",
        "          top_score2 = np.sum(top_emb * title_emb, axis=1) / np.linalg.norm(title_emb, axis=1)\n",
        "       \n",
        "        query_emb =  bc.encode([query])[0]\n",
        "       \n",
        "        score = np.sum(query_emb * title_emb, axis=1) / np.linalg.norm(title_emb, axis=1)\n",
        "        \n",
        "        for i,s in enumerate(score):\n",
        "          f.write(str(qid) + \",\" + str(dids[i]) + \",\" + str(s) + \",\" + str(top_score[i]) + \",\" + str(top_score2[i]) + \"\\n\")\n",
        "      if qid%100==0:\n",
        "        if qid==1200 :\n",
        "          flag=True\n",
        "        if(flag):\n",
        "          print(qid, score)\n",
        "    except Exception as e:\n",
        "      print(repr(e), qid)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}