{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "def parseTrainTestFile(train_marks_filename, isTest=False):\n",
    "\n",
    "    trainDidToLine = {}\n",
    "    qidToDocs = defaultdict(list)\n",
    "    with open(train_marks_filename) as f:\n",
    "        i=0\n",
    "        nlines = 0\n",
    "        for line in f:\n",
    "            nlines+=1\n",
    "        \n",
    "       \n",
    "        qids = np.zeros( nlines, dtype=int)\n",
    "        i=0\n",
    "        f.seek(0)\n",
    "        if isTest:\n",
    "            f.readline()\n",
    "        for ii,line in enumerate(f):\n",
    "            if not isTest:\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts)!=3:\n",
    "                    continue\n",
    "                qid, did, mark = parts\n",
    "                qid, did, mark = int(qid), int(did), int(mark)\n",
    "            else:\n",
    "                parts = line.split(\",\")\n",
    "                if len(parts)!=2:\n",
    "                    continue\n",
    "                qid, did = parts\n",
    "                mark=-1\n",
    "                qid, did, mark = int(qid), int(did), int(mark)\n",
    "\n",
    "            qids[i] = qid\n",
    "            trainDidToLine[(qid,did)] = i\n",
    "\n",
    "            qidToDocs[qid].append(did)\n",
    "            i+=1\n",
    "    return qids, trainDidToLine, qidToDocs\n",
    "q_train, trainDidToLine, trainQidToDocs = parseTrainTestFile(\"train.marks.tsv\", \n",
    "                                                    isTest=False, \n",
    "                                                    )\n",
    "\n",
    "q_test, testDidToLine, testQidToDocs = parseTrainTestFile(\"sample.csv\", \n",
    "                                                    isTest=True, \n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi, BM25Plus, BM25L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bms = [BM25Okapi, BM25Plus, BM25L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_corpus = []\n",
    "dids = []\n",
    "didToLine = {}\n",
    "with open(\"../data/normal_titles.txt\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        did,title = line.split(\"\\t\")\n",
    "        did = int(did)\n",
    "        words = title.split(\" \")\n",
    "        titles_corpus.append(words)\n",
    "        dids.append(did)\n",
    "        didToLine[did] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_corpus = []\n",
    "qids = []\n",
    "qidToLine = {}\n",
    "with open(\"../data/normal_queries.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        qid,title = line.split(\"\\t\")\n",
    "        qid = int(qid)\n",
    "        words = title.split(\" \")\n",
    "        queries_corpus.append(words)\n",
    "        qidToLine[qid] = len(qids)\n",
    "        qids.append(qid)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_corpus=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bmid,bm in enumerate(bms):\n",
    "    bm25 = bm(titles_corpus)\n",
    "    with open(\"train_bm25_\" + str(bmid) + \".txt\", \"w\") as f:\n",
    "        #qidToDocsScores = defaultdict(list)\n",
    "        cnt=0\n",
    "        for qid,dids in trainQidToDocs.items():\n",
    "            scores = bm25.get_scores(queries_corpus[qidToLine[qid]])\n",
    "            for did in dids:\n",
    "                if did in didToLine:\n",
    "                    did_line = didToLine[did]\n",
    "                    #qidToDocsScores[qid].append( (did,scores[did_line]))\n",
    "                    f.write(str(qid) + \",\" + str(did) + \",\" + str(scores[did_line]) + \"\\n\")\n",
    "\n",
    "            cnt+=1\n",
    "            if(cnt%200==0):\n",
    "                print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bmid,bm in enumerate(bms):\n",
    "    bm25 = bm(titles_corpus)\n",
    "    with open(\"test_bm25_\" + str(bmid) + \".txt\", \"w\") as f:\n",
    "        #qidToDocsScores = defaultdict(list)\n",
    "        cnt=0\n",
    "        for qid,dids in testQidToDocs.items():\n",
    "            scores = bm25.get_scores(queries_corpus[qidToLine[qid]])\n",
    "            for did in dids:\n",
    "                  if did in didToLine:\n",
    "                    did_line = didToLine[did]\n",
    "\n",
    "                    #qidToDocsScores[qid].append( (did,scores[did_line]))\n",
    "                    f.write(str(qid) + \",\" + str(did) + \",\" + str(scores[did_line]) + \"\\n\")\n",
    "\n",
    "            cnt+=1\n",
    "            if(cnt%200==0):\n",
    "                print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_normalize(url):\n",
    "    url = url.strip()\n",
    "    url = url.replace(\"https://\", \"\")\n",
    "    url = url.replace(\"http://\", \"\")\n",
    "    if len(url)>1:\n",
    "        if url[-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "        url = url.lower()\n",
    "        url = url.replace(\":80\", \"\")\n",
    "        url = url.replace(\"www.\", \"\")\n",
    "        url = url.replace(\"//\", \"\")\n",
    "    return url\n",
    "def extract_domain(url):\n",
    "    global rubrics_list\n",
    "    url = url.strip()\n",
    "    #for r in rubrics_list:\n",
    "        #url = url.replace(\",\" + r + \",\" , \"\")    \n",
    "    #    url = url.replace(\",\" + r, \"\") \n",
    "    url = url.replace(\"https://\", \"\")\n",
    "    url = url.replace(\"http://\", \"\")\n",
    "   \n",
    "    url = url.replace(\"ftp://\", \"\")\n",
    "    if len(url)>1:\n",
    "        if url[-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "        url = url.lower()\n",
    "        \n",
    "        url = url.replace(\"s://\", \"\")\n",
    "        url = url.replace(\":80\", \"\")\n",
    "        url = url.replace(\"://\", \"\")\n",
    "        url = url.replace(\"www.\", \"\")\n",
    "        url = url.replace(\"//\", \"\")\n",
    "        pos = url.find(\"#\")\n",
    "        if pos>0:\n",
    "            url = url[0:pos]\n",
    "\n",
    "        pos = url.find(\"/\")\n",
    "        if pos>0:\n",
    "            url = url[0:pos]\n",
    "    return url\n",
    "def loadUrls(filename):\n",
    "    urlToDid = {}\n",
    "    didToUrl = {}\n",
    "    domainToDid = defaultdict(list)\n",
    "    didToDomain = defaultdict(list)\n",
    "   \n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            uid, url = line.split(\"\\t\")\n",
    "            uid = int(uid)\n",
    "            #urls[\"http://\" + url] = uid\n",
    "            #urls[\"http://\" + url + \"/\"] = uid\n",
    "            urlToDid[url_normalize(url)] = uid\n",
    "            didToUrl[uid] = url_normalize(url)\n",
    "            domain = extract_domain(url)\n",
    "            domainToDid[domain].append(uid)\n",
    "            didToDomain[uid] = domain\n",
    "            \n",
    "    return urlToDid,didToUrl,domainToDid,didToDomain\n",
    "urlToDid,didToUrl,domainToDid,didToDomain = loadUrls(\"url.data\")\n",
    "urls = urlToDid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "not_matched = 0\n",
    "nqueries=0\n",
    "def tmp():\n",
    "    return [10000,10000]\n",
    "cnt=0\n",
    "not_matched = 0\n",
    "nqueries=0\n",
    "\n",
    "def loadTopDocsFile(filename):\n",
    "    found_docs = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            qid,fd = line.split(\",\")\n",
    "            qid = int(qid)\n",
    "            fd = int(fd)\n",
    "            found_docs[qid] = (fd,0.0)\n",
    "    return found_docs\n",
    "found_docs = loadTopDocsFile(\"first_top_docs.txt\")\n",
    "found_domains = loadTopDocsFile(\"first_top_domains.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titlesGen():\n",
    "   \n",
    "    for i,t in enumerate(titles_corpus):\n",
    "        s = \"\"\n",
    "        for w in t:\n",
    "            s+=w + \" \"\n",
    "        yield s\n",
    "        if i% (len(titles_corpus)//100) == 0:\n",
    "            print( i/len(titles_corpus))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=['english'], analyzer = \"char\", ngram_range=(2,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_tfidf = vectorizer.fit_transform(titlesGen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queriesGen():\n",
    "   \n",
    "    for i,t in enumerate(queries_corpus):\n",
    "        s = \"\"\n",
    "        for w in t:\n",
    "            s+=w + \" \"\n",
    "        yield s\n",
    "        if i% (len(queries_corpus)//100) == 0:\n",
    "            print( i/len(queries_corpus))\n",
    "   \n",
    "queries_tfidf = vectorizer.transform(queriesGen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_tdidf_ngrams_w.txt\", \"w\") as f:\n",
    "    \n",
    "    cnt=0\n",
    "    for qid,dids in trainQidToDocs.items():\n",
    "       \n",
    "        for did in dids:\n",
    "            if did in didToLine:\n",
    "                did_line = didToLine[did]\n",
    "                score = cosine_similarity(queries_tfidf[qidToLine[qid]], titles_tfidf[did_line] )\n",
    "\n",
    "                #qidToDocsScores[qid].append( (did,scores[did_line]))\n",
    "                f.write(str(qid) + \",\" + str(did) + \",\" + str(score[0][0]) + \"\\n\")\n",
    "\n",
    "        cnt+=1\n",
    "        if(cnt%200==0):\n",
    "            print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_tdidf_ngrams_w.txt\", \"w\") as f:\n",
    "    #qidToDocsScores = defaultdict(list)\n",
    "    cnt=0\n",
    "    for qid,dids in testQidToDocs.items():\n",
    "       \n",
    "        for did in dids:\n",
    "            if did in didToLine:\n",
    "                did_line = didToLine[did]\n",
    "                score = cosine_similarity(queries_tfidf[qidToLine[qid]], titles_tfidf[did_line] )\n",
    "\n",
    "                #qidToDocsScores[qid].append( (did,scores[did_line]))\n",
    "                f.write(str(qid) + \",\" + str(did) + \",\" + str(score[0][0]) + \"\\n\")\n",
    "\n",
    "        cnt+=1\n",
    "        if(cnt%200==0):\n",
    "            print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "stopWords = list(stopwords.words('russian')) + list(stopwords.words('english'))\n",
    "vectorizer = HashingVectorizer(stop_words=stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "didtoline = []\n",
    "import os\n",
    "\n",
    "def docsGen2():\n",
    "    doc_files = os.listdir(\"../normalize_docs/\")\n",
    "    doc_files = list(map(lambda x: \"../normalize_docs/\"+x, docs))\n",
    "    global didtoline\n",
    "    didtoline = []\n",
    "    cnt=0\n",
    "    for filename in doc_files:\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                parts = line.split(\"\\t\", maxsplit=1)\n",
    "                if len(parts)==2:\n",
    "                    did = int(parts[0])\n",
    "                    didtoline.append(did)\n",
    "                    content = parts[1]\n",
    "                    yield did,content\n",
    "                    if(cnt%1000==0):\n",
    "                        print(cnt)\n",
    "                    cnt+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
